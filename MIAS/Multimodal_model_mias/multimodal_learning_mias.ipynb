{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 5967,
          "sourceType": "datasetVersion",
          "datasetId": 3748
        },
        {
          "sourceId": 10371269,
          "sourceType": "datasetVersion",
          "datasetId": 6424199
        },
        {
          "sourceId": 432900,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 352922,
          "modelId": 374204
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Final Exp on MIAS",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "oAl9xCBnIjvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import glob\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import densenet121\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, DeiTForImageClassification\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:04:40.953138Z",
          "iopub.execute_input": "2025-06-18T04:04:40.953385Z",
          "iopub.status.idle": "2025-06-18T04:04:49.476484Z",
          "shell.execute_reply.started": "2025-06-18T04:04:40.953364Z",
          "shell.execute_reply": "2025-06-18T04:04:49.475941Z"
        },
        "id": "fg4v998rHkRW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seeding"
      ],
      "metadata": {
        "id": "tfNnJdyOI24S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 42"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:04:54.369034Z",
          "iopub.execute_input": "2025-06-18T04:04:54.369834Z",
          "iopub.status.idle": "2025-06-18T04:04:54.374402Z",
          "shell.execute_reply.started": "2025-06-18T04:04:54.369809Z",
          "shell.execute_reply": "2025-06-18T04:04:54.373685Z"
        },
        "id": "oDjeBPWyHkRX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links and device"
      ],
      "metadata": {
        "id": "Gxg5D-jLJFxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "url = \"/kaggle/input/mias-mammography/all-mias/\"  # Base URL for data\n",
        "csv_file_path = '/kaggle/input/mias-text-without-co-ordinates/MIAS text.csv'\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:04:57.368667Z",
          "iopub.execute_input": "2025-06-18T04:04:57.369174Z",
          "iopub.status.idle": "2025-06-18T04:04:57.372517Z",
          "shell.execute_reply.started": "2025-06-18T04:04:57.36915Z",
          "shell.execute_reply": "2025-06-18T04:04:57.371869Z"
        },
        "id": "pQfitUwaHkRX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:01.303372Z",
          "iopub.execute_input": "2025-06-18T04:05:01.303665Z",
          "iopub.status.idle": "2025-06-18T04:05:01.375989Z",
          "shell.execute_reply.started": "2025-06-18T04:05:01.303643Z",
          "shell.execute_reply": "2025-06-18T04:05:01.375307Z"
        },
        "id": "dZAg4edOHkRX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image data preprocessing"
      ],
      "metadata": {
        "id": "C-E2-IoZJJw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to isolate breast region\n",
        "def preprocess_image_2(img):\n",
        "    \"\"\"Preprocess a mammogram image to isolate the breast region.\"\"\"\n",
        "    _, binary_img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    morphed_img = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(morphed_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "    mask = np.zeros_like(binary_img)\n",
        "    cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
        "    isolated_breast = cv2.bitwise_and(img, img, mask=mask)\n",
        "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
        "    cropped_breast = isolated_breast[y:y+h, x:x+w]\n",
        "    return cropped_breast"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:03.647964Z",
          "iopub.execute_input": "2025-06-18T04:05:03.648878Z",
          "iopub.status.idle": "2025-06-18T04:05:03.654212Z",
          "shell.execute_reply.started": "2025-06-18T04:05:03.648848Z",
          "shell.execute_reply": "2025-06-18T04:05:03.653509Z"
        },
        "id": "FZchFgU0HkRY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply bicubic interpolation\n",
        "def apply_bicubic_interpolation(image, scale_factor=2):\n",
        "    \"\"\"Enhance an image using bicubic interpolation-based super-resolution.\"\"\"\n",
        "    height, width = image.shape[:2]\n",
        "    new_height, new_width = int(height * scale_factor), int(width * scale_factor)\n",
        "    high_res_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # Resize the image to ensure it has the same shape (224, 224)\n",
        "    high_res_image = cv2.resize(high_res_image, (224, 224))\n",
        "    high_res_image = np.expand_dims(high_res_image, axis=-1)\n",
        "\n",
        "    return high_res_image"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:07.878198Z",
          "iopub.execute_input": "2025-06-18T04:05:07.878501Z",
          "iopub.status.idle": "2025-06-18T04:05:07.883543Z",
          "shell.execute_reply.started": "2025-06-18T04:05:07.87845Z",
          "shell.execute_reply": "2025-06-18T04:05:07.882623Z"
        },
        "id": "LkI9i3d6HkRY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and preprocess images\n",
        "def read_image():\n",
        "    \"\"\"Read and preprocess images without augmentation.\"\"\"\n",
        "    print(\"Reading images\")\n",
        "    info = {}  # Dictionary to store image data\n",
        "\n",
        "    for i in range(322):  # 322 images in total\n",
        "        if i < 9:\n",
        "            image_name = f'mdb00{i + 1}'\n",
        "        elif i < 99:\n",
        "            image_name = f'mdb0{i + 1}'\n",
        "        else:\n",
        "            image_name = f'mdb{i + 1}'\n",
        "\n",
        "        image_address = os.path.join(url, f\"{image_name}.pgm\")\n",
        "        img = cv2.imread(image_address, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Check if the image exists\n",
        "        if img is not None:\n",
        "            info[image_name] = img  # Store image directly\n",
        "        else:\n",
        "            print(f\"Warning: Image {image_name} not found.\")\n",
        "\n",
        "    print(f\"Total images read: {len(info)}\")  # Debugging the number of images read\n",
        "    return info\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:11.299759Z",
          "iopub.execute_input": "2025-06-18T04:05:11.300221Z",
          "iopub.status.idle": "2025-06-18T04:05:11.305105Z",
          "shell.execute_reply.started": "2025-06-18T04:05:11.300197Z",
          "shell.execute_reply": "2025-06-18T04:05:11.304507Z"
        },
        "id": "-9M8YGK5HkRY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read labels from file\n",
        "def read_label():\n",
        "    \"\"\"Read labels from file.\"\"\"\n",
        "    print(\"Reading labels\")\n",
        "    filename = url + 'Info.txt'\n",
        "    text_all = open(filename).read()\n",
        "    lines = text_all.split('\\n')\n",
        "    info = {}  # Dictionary for label data\n",
        "\n",
        "    for line in lines:\n",
        "        words = line.split(' ')\n",
        "        if len(words) > 3:\n",
        "            if (words[3] == 'B'):  # Label 'B' for benign\n",
        "                info[words[0]] = 0  # Assigning label 0 for benign\n",
        "            if (words[3] == 'M'):  # Label 'M' for malignant\n",
        "                info[words[0]] = 1  # Assigning label 1 for malignant\n",
        "\n",
        "    return info"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:14.873847Z",
          "iopub.execute_input": "2025-06-18T04:05:14.874509Z",
          "iopub.status.idle": "2025-06-18T04:05:14.879067Z",
          "shell.execute_reply.started": "2025-06-18T04:05:14.874485Z",
          "shell.execute_reply": "2025-06-18T04:05:14.878274Z"
        },
        "id": "OMrC0tVNHkRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load label data\n",
        "label_info = read_label()\n",
        "image_info = read_image()\n",
        "\n",
        "# Ensure that ids are properly aligned\n",
        "ids = list(label_info.keys())\n",
        "\n",
        "# Remove 'Truth-Data:' from label information if it exists\n",
        "if 'Truth-Data:' in label_info:\n",
        "    del label_info['Truth-Data:']\n",
        "\n",
        "# Print the number of labels\n",
        "print(f\"Total number of labels: {len(label_info)}\")\n",
        "\n",
        "# Prepare X and Y arrays\n",
        "X, Y = [], []\n",
        "\n",
        "# Check for images without corresponding labels\n",
        "missing_labels = []\n",
        "\n",
        "# Loop through image names to handle missing labels and apply preprocessing and bicubic interpolation\n",
        "for id in image_info.keys():  # Loop through image names\n",
        "    if id in label_info:  # If label exists for the image\n",
        "        # Apply preprocessing\n",
        "        preprocessed_image = preprocess_image_2(image_info[id])\n",
        "\n",
        "        # Apply bicubic interpolation\n",
        "        high_res_image = apply_bicubic_interpolation(preprocessed_image, scale_factor=2)\n",
        "\n",
        "        # Store the processed and high-res images\n",
        "        X.append(high_res_image)\n",
        "        Y.append(label_info[id])\n",
        "    else:  # If no label for the image\n",
        "        missing_labels.append(id)\n",
        "        # Apply preprocessing\n",
        "        preprocessed_image = preprocess_image_2(image_info[id])\n",
        "\n",
        "        # Apply bicubic interpolation\n",
        "        high_res_image = apply_bicubic_interpolation(preprocessed_image, scale_factor=2)\n",
        "\n",
        "        # Assign default label 'N' for missing labels (Benign)\n",
        "        X.append(high_res_image)\n",
        "        Y.append(0)  # Default 'Benign' label\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "# Print dataset size to check if everything is correct\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"Y shape: {Y.shape}\")\n",
        "\n",
        "# Print missing labels (if any)\n",
        "if missing_labels:\n",
        "    print(f\"Images without corresponding labels: {missing_labels}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:18.753553Z",
          "iopub.execute_input": "2025-06-18T04:05:18.754188Z",
          "iopub.status.idle": "2025-06-18T04:05:33.397014Z",
          "shell.execute_reply.started": "2025-06-18T04:05:18.754169Z",
          "shell.execute_reply": "2025-06-18T04:05:33.396244Z"
        },
        "id": "pQOOtRzZHkRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation"
      ],
      "metadata": {
        "id": "bKMjmlUjKLZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define updated transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to a fixed size\n",
        "    transforms.RandomAffine(degrees=60,  # Rotation up to ±60 degrees\n",
        "                            translate=(0.1, 0.1),  # Random translation up to 10% of the image size\n",
        "                            scale=(0.8, 1.2),  # Random scaling between 80% and 120%\n",
        "                            shear=20),  # Random shearing up to ±20 degrees\n",
        "    transforms.ColorJitter(contrast=1.0),  # Enhance image contrast\n",
        "    transforms.ToTensor(),  # Convert to Tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:41.678838Z",
          "iopub.execute_input": "2025-06-18T04:05:41.679098Z",
          "iopub.status.idle": "2025-06-18T04:05:41.684774Z",
          "shell.execute_reply.started": "2025-06-18T04:05:41.67908Z",
          "shell.execute_reply": "2025-06-18T04:05:41.684032Z"
        },
        "id": "MiN7d92CHkRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare text data (CSV file)\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "texts = df['Generated Sentence'].values\n",
        "\n",
        "# Ensure data lengths match\n",
        "assert len(X) == len(Y) == len(texts), \"Mismatch between X, Y, and texts lengths before augmentation!\"\n",
        "\n",
        "# Step 1: Find the maximum class count\n",
        "class_counts = Counter(Y)\n",
        "max_count = max(class_counts.values())\n",
        "\n",
        "# Step 2: Convert grayscale images to RGB\n",
        "X_rgb = np.stack([cv2.cvtColor(img.squeeze(), cv2.COLOR_GRAY2RGB) for img in X])\n",
        "print(f\"Converted X_rgb shape: {X_rgb.shape}\")\n",
        "\n",
        "# Step 3: Initialize augmented images, labels, and texts\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "augmented_texts = []\n",
        "\n",
        "# Step 4: Augment the minority classes until all are balanced\n",
        "for label, count in class_counts.items():\n",
        "    if count < max_count:\n",
        "        # Get the indices of images and texts of the minority class from the original dataset\n",
        "        minority_class_indices = np.where(Y == label)[0]\n",
        "        images_to_augment = X_rgb[minority_class_indices]\n",
        "        texts_to_augment = np.array(texts)[minority_class_indices]\n",
        "\n",
        "        # Augment until the count matches max_count\n",
        "        while count < max_count:\n",
        "            for img, text in zip(images_to_augment, texts_to_augment):\n",
        "                if count >= max_count:\n",
        "                    break\n",
        "                # Apply augmentation\n",
        "                pil_img = Image.fromarray(img.astype(np.uint8))\n",
        "                augmented_img = train_transform(pil_img)  # Apply transformations\n",
        "\n",
        "                # Append augmented image, label, and corresponding text\n",
        "                augmented_images.append(\n",
        "                    np.array(augmented_img.permute(1, 2, 0))  # Convert back to numpy\n",
        "                )\n",
        "                augmented_labels.append(label)\n",
        "                augmented_texts.append(text)\n",
        "                count += 1\n",
        "\n",
        "# Step 5: Convert augmented data to numpy arrays\n",
        "if augmented_images:\n",
        "    augmented_images = np.array(augmented_images)\n",
        "    augmented_labels = np.array(augmented_labels)\n",
        "    augmented_texts = np.array(augmented_texts)\n",
        "\n",
        "    # Check augmentation results\n",
        "    print(f\"Number of augmented images: {len(augmented_images)}\")\n",
        "    print(f\"Number of augmented labels: {len(augmented_labels)}\")\n",
        "    print(f\"Number of augmented texts: {len(augmented_texts)}\")\n",
        "\n",
        "    # Step 6: Concatenate augmented data with original training data\n",
        "    X_rgb = np.concatenate((X_rgb, augmented_images), axis=0)\n",
        "    Y = np.concatenate((Y, augmented_labels), axis=0)\n",
        "    A_texts = np.concatenate((texts, augmented_texts), axis=0)\n",
        "\n",
        "# Final Check: Validate shapes\n",
        "assert X_rgb.shape[0] == Y.shape[0] == len(A_texts), (\n",
        "    f\"Mismatch after augmentation: {X_rgb.shape[0]} vs {Y.shape[0]} vs {len(A_texts)}\"\n",
        ")\n",
        "print(f\"Shapes after augmentation:\\nX_rgb shape: {X_rgb.shape}\\nY shape: {Y.shape}\\nTexts count: {len(A_texts)}\")\n",
        "\n",
        "# Step 7: Check the new class distribution\n",
        "new_class_counts = Counter(Y)\n",
        "print(\"Class distribution after augmentation:\", new_class_counts)\n",
        "\n",
        "# Step 8: Plot the class distribution\n",
        "counts = [new_class_counts[i] for i in sorted(new_class_counts.keys())]\n",
        "class_names = ['Benign', 'Malignant']  # Update with your actual class names\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(class_names, counts, color=['blue', 'red'])\n",
        "plt.title(\"Class Distribution After Augmentation\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T04:05:45.57868Z",
          "iopub.execute_input": "2025-06-18T04:05:45.579207Z",
          "iopub.status.idle": "2025-06-18T04:05:46.575948Z",
          "shell.execute_reply.started": "2025-06-18T04:05:45.579185Z",
          "shell.execute_reply": "2025-06-18T04:05:46.5753Z"
        },
        "id": "VHWLqyGpHkRZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional Gated Cross-Attention"
      ],
      "metadata": {
        "id": "Z5Ed5qCeN8oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedCrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim, hidden_dim):\n",
        "        super(GatedCrossAttention, self).__init__()\n",
        "        self.query_proj = nn.Linear(query_dim, hidden_dim)\n",
        "        self.key_proj = nn.Linear(context_dim, hidden_dim)\n",
        "        self.value_proj = nn.Linear(context_dim, hidden_dim)\n",
        "\n",
        "        # Gating mechanism\n",
        "        self.gate_fc = nn.Linear(query_dim + hidden_dim, hidden_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, query, context):\n",
        "        Q = self.query_proj(query).unsqueeze(1)     # [B, 1, H]\n",
        "        K = self.key_proj(context).unsqueeze(1)     # [B, 1, H]\n",
        "        V = self.value_proj(context).unsqueeze(1)   # [B, 1, H]\n",
        "\n",
        "        attn_scores = torch.bmm(Q, K.transpose(1, 2))  # [B, 1, 1]\n",
        "        attn_weights = self.softmax(attn_scores)       # [B, 1, 1]\n",
        "        attended = torch.bmm(attn_weights, V).squeeze(1)  # [B, H]\n",
        "\n",
        "        # Project query into hidden space for fusion\n",
        "        query_proj = self.query_proj(query)  # [B, H]\n",
        "\n",
        "        # Gate computation\n",
        "        gate_input = torch.cat([query, attended], dim=1)  # [B, Q+H]\n",
        "        gate = self.sigmoid(self.gate_fc(gate_input))     # [B, H]\n",
        "\n",
        "        # Gated fusion\n",
        "        gated_output = gate * query_proj + (1 - gate) * attended  # [B, H]\n",
        "        return gated_output"
      ],
      "metadata": {
        "id": "sKHiXwMBOBwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal model"
      ],
      "metadata": {
        "id": "G24i36gDONLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, efficientvit_model, deit_model, text_model, fc_network):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.efficientvit_model = efficientvit_model\n",
        "        self.deit_model = deit_model\n",
        "        self.text_model = text_model\n",
        "        self.fc_network = fc_network\n",
        "\n",
        "        self.image_feature_dim = 1024\n",
        "        self.deit_feature_dim = 768\n",
        "        self.text_feature_dim = 768  # **Fixed: Correct text feature dimension**\n",
        "        self.hidden_dim = 768\n",
        "\n",
        "        self.text_to_vision = GatedCrossAttention(self.text_feature_dim, self.image_feature_dim + self.deit_feature_dim, self.hidden_dim)\n",
        "        self.vision_to_text = GatedCrossAttention(self.image_feature_dim + self.deit_feature_dim, self.text_feature_dim, self.hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, image_input, input_ids, attention_mask):\n",
        "        # Extract features from EfficientViT and DeiT\n",
        "        image_features = self.efficientvit_model(image_input)  # [batch, 1024]\n",
        "        deit_features = self.deit_model(image_input).logits   # [batch, 768]\n",
        "        vision_features = torch.cat((image_features, deit_features), dim=1)  # **Shape: [batch, 1792]**\n",
        "\n",
        "        text_outputs = self.text_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        text_features = text_outputs.hidden_states[-1][:, 0, :]  # **Extract hidden state ([batch, 2])**\n",
        "        # print(\"Text Features Shape:\", text_features.shape)\n",
        "        # print(\"Vision Features Shape:\", vision_features.shape)\n",
        "\n",
        "\n",
        "        # Fix mismatch: Change text feature projection size\n",
        "        enhanced_text = self.text_to_vision(text_features, vision_features)\n",
        "        enhanced_vision = self.vision_to_text(vision_features, text_features)\n",
        "\n",
        "        fused_features = torch.cat([enhanced_text, enhanced_vision, vision_features[:, :1024]], dim=1)\n",
        "\n",
        "        output = self.fc_network(fused_features)  # Input size matches fc_network\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "ARXqkhvtORMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully connected network"
      ],
      "metadata": {
        "id": "GdQa-oe-Olvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnectedNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FullyConnectedNetwork, self).__init__()\n",
        "\n",
        "        # Define the layers for the fully connected network\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
        "\n",
        "        # BatchNorm and ReLU layers\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_dim // 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply layers sequentially\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7svu6iN7Oo4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image feature dimension\n",
        "image_feature_dim = 1024  # Adjust based on EfficientViT output\n",
        "deit_feature_dim = 768  # DeiT feature output\n",
        "text_feature_dim = 768   # Adjust based on MedBERT output (equal to `num_labels` in classification tasks)\n",
        "hidden_dim = 512\n",
        "output_dim = 2  # Number of classes\n",
        "\n",
        "# Create the fully connected network\n",
        "fc_network = FullyConnectedNetwork(\n",
        "    input_dim=image_feature_dim +deit_feature_dim + text_feature_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    output_dim=output_dim\n",
        ")"
      ],
      "metadata": {
        "id": "7FY-k3mHO1Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal dataset"
      ],
      "metadata": {
        "id": "6Ezorp1OPOwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, texts, labels, tokenizer, transform=None, max_text_length=512):\n",
        "        self.images = images\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_text_length = max_text_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Process image\n",
        "        image = self.images[idx]\n",
        "\n",
        "        # Convert numpy array to PIL image if it's in numpy array format\n",
        "        if isinstance(image, np.ndarray):\n",
        "            # Ensure the image is in the correct data type (uint8)\n",
        "            if image.dtype != np.uint8:\n",
        "                image = (image * 255).astype(np.uint8)\n",
        "            # Convert to PIL Image\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply image transformations\n",
        "\n",
        "        # Process text\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_text_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze(0)  # Remove batch dimension\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)  # Remove batch dimension\n",
        "\n",
        "        # Get label\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"label\": label\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "metadata": {
        "id": "eWuAtLUAPRH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class weights"
      ],
      "metadata": {
        "id": "aW-0MpqCQ0P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=torch.unique(torch.tensor(labels)).numpy(),\n",
        "    y=labels\n",
        ")\n",
        "\n",
        "# Convert to tensor for PyTorch\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Move class weights to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(f\"Class Weights on Device: {class_weights_tensor}\")\n"
      ],
      "metadata": {
        "id": "O1CC2Q3OQhs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train loop"
      ],
      "metadata": {
        "id": "kxZxlmNUPhSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        # Move data to the device\n",
        "        images = batch[\"image\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct_predictions += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    # Step the scheduler based on the validation loss (if using ReduceLROnPlateau)\n",
        "    # scheduler.step(epoch_loss)  # Uncomment if using ReduceLROnPlateau\n",
        "\n",
        "    # Step the scheduler every epoch (if using StepLR or similar)\n",
        "    scheduler.step()\n",
        "\n",
        "    return epoch_loss, epoch_accuracy\n"
      ],
      "metadata": {
        "id": "iSlptK1qPpZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation loop"
      ],
      "metadata": {
        "id": "tyQ5nVxAPq8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            # Move data to the device\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    true_labels = torch.stack(true_labels).cpu()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    return epoch_loss, epoch_accuracy, classification_report(\n",
        "        true_labels, predictions, target_names=df['Class'].unique(), output_dict=True\n",
        "    )"
      ],
      "metadata": {
        "id": "37p8-ezpPugo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion matrix"
      ],
      "metadata": {
        "id": "3QLbJCaXP2H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_confusion_matrix(true_labels, predictions, save_cm_path, class_names=['Non-malignant', 'Malignant']):\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(save_cm_path), exist_ok=True)\n",
        "\n",
        "    # Generate Confusion Matrix\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names,\n",
        "                yticklabels=class_names, linewidths=2, cbar=False, square=True, annot_kws={\"size\": 14})\n",
        "\n",
        "    plt.xlabel('Predicted Labels', fontsize=12)\n",
        "    plt.ylabel('True Labels', fontsize=12)\n",
        "    plt.title('Confusion Matrix', fontsize=14)\n",
        "\n",
        "    # Save confusion matrix as PDF\n",
        "    plt.savefig(save_cm_path, bbox_inches=\"tight\", format=\"pdf\")\n",
        "    plt.close()\n",
        "    print(f\"Confusion matrix saved as {save_cm_path}\")\n",
        "\n",
        "    # Return classification report\n",
        "    return classification_report(true_labels, predictions, target_names=class_names, output_dict=True)\n"
      ],
      "metadata": {
        "id": "orkLe-QxP6p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test loop"
      ],
      "metadata": {
        "id": "lA20EHUAQSF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, val_loader, criterion, device, seed=None, report_save_path=None):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluation\"):\n",
        "            images = batch[\"image\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            outputs = model(images, input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "            correct_predictions += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            predictions.extend(preds.cpu())\n",
        "            true_labels.extend(labels.cpu())\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            probabilities.extend(probs.cpu())\n",
        "\n",
        "    predictions = torch.stack(predictions)\n",
        "    true_labels = torch.stack(true_labels)\n",
        "    probabilities = torch.stack(probabilities)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    auc_roc = roc_auc_score(true_labels, probabilities[:, 1], multi_class=\"ovr\") if probabilities.shape[1] > 1 else None\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    if auc_roc is not None:\n",
        "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    save_cm_path = '/kaggle/working/confusion_matrix.pdf'\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Non-malignant', 'Malignant'],\n",
        "                yticklabels=['Non-malignant', 'Malignant'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(save_cm_path)\n",
        "    plt.show()\n",
        "\n",
        "    # Classification Report\n",
        "    class_report = classification_report(\n",
        "        true_labels,\n",
        "        predictions,\n",
        "        target_names=['Non-malignant', 'Malignant'],\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    # Return metrics summary and full classification report\n",
        "    results_df = pd.DataFrame([{\n",
        "        'Seed': seed if seed is not None else 'N/A',\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'AUC-ROC': auc_roc\n",
        "    }])\n",
        "\n",
        "    return results_df, class_report"
      ],
      "metadata": {
        "id": "tE4g3nFJP10i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal learning (DenseNet121+Deit+BERT)"
      ],
      "metadata": {
        "id": "aS1AsG6ZMEwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DenseNet121\n",
        "def load_densenet_model(weight_path):\n",
        "\n",
        "    densenet_model = densenet121(pretrained=False)\n",
        "    densenet_model.classifier = torch.nn.Identity()\n",
        "\n",
        "    # Load the state_dict\n",
        "    state_dict = torch.load(weight_path)\n",
        "\n",
        "    # Remove \"classifier.weight\" and \"classifier.bias\" from the state_dict\n",
        "    state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"classifier.\")}\n",
        "\n",
        "    # Load the pruned state_dict into the model\n",
        "    densenet_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    densenet_model.eval()\n",
        "\n",
        "    for param in densenet_model.parameters():\n",
        "        param.requires_grad = False  # Freeze all parameters\n",
        "\n",
        "    return densenet_model\n",
        "\n",
        "# Load DeiT model\n",
        "def load_deit_model(weight_path, num_classes=2):\n",
        "\n",
        "    deit_model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n",
        "\n",
        "    # Modify the classifier layer\n",
        "    deit_model.classifier = nn.Identity()\n",
        "\n",
        "    # Load trained weights\n",
        "    state_dict = torch.load(weight_path, map_location=torch.device('cpu'))\n",
        "    deit_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    deit_model.eval()\n",
        "\n",
        "    # Freeze parameters\n",
        "    for param in deit_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return deit_model\n",
        "\n",
        "# Load the BERT model\n",
        "def load_bert_model(weight_path, bert_model_name=\"Charangan/MedBERT\"):\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "    # Initialize the model with the same configuration used during training\n",
        "    bert_model = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=2)\n",
        "\n",
        "    # Load the state_dict into the model\n",
        "    state_dict = torch.load(weight_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    # Load the state dict into the model (ignore mismatched keys if any)\n",
        "    bert_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    bert_model.eval()\n",
        "\n",
        "    # Freeze the parameters of the model (for feature extraction)\n",
        "    for param in bert_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    return bert_model, tokenizer\n",
        "\n",
        "# Paths to Kaggle dataset files\n",
        "densenet_weight_path = '/kaggle/input/best-models-with-seeds/pytorch/default/1/DenseNet_best_model.pt'\n",
        "deit_weight_path = '/kaggle/input/best-models-with-seeds/pytorch/default/1/deit_best_model.bin'\n",
        "bert_weight_path = '/kaggle/input/best-models-with-seeds/pytorch/default/1/best_BERT_model_state.bin'\n",
        "\n",
        "# Load the pretrained models for feature extraction\n",
        "densenet_model = load_densenet_model(densenet_weight_path)\n",
        "deit_model = load_deit_model(deit_weight_path)\n",
        "text_model, bert_tokenizer = load_bert_model(bert_weight_path)\n",
        "\n",
        "print(\"Models loaded and ready for feature extraction.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T05:52:15.077543Z",
          "iopub.execute_input": "2025-06-18T05:52:15.077847Z",
          "iopub.status.idle": "2025-06-18T05:52:22.985977Z",
          "shell.execute_reply.started": "2025-06-18T05:52:15.077823Z",
          "shell.execute_reply": "2025-06-18T05:52:22.985314Z"
        },
        "id": "qCYwVxc0HkRa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments across 10 different seeds"
      ],
      "metadata": {
        "id": "PixbCRwSHkRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments_over_seeds(seed_list):\n",
        "    all_val_reports = []\n",
        "    all_test_reports = []\n",
        "    # Initialize a list to store AUC-ROC scores for each seed\n",
        "    all_test_auc_roc_scores = []\n",
        "\n",
        "    for seed in seed_list:\n",
        "        print(f\"\\n====== Running for Seed: {seed} ======\")\n",
        "\n",
        "        # Set seed\n",
        "        set_seed(seed)\n",
        "        def seed_worker(worker_id): np.random.seed(seed); random.seed(seed)\n",
        "\n",
        "        # Reload all models fresh for this seed\n",
        "        densenet_model = load_densenet_model(densenet_weight_path)\n",
        "        deit_model = load_deit_model(deit_weight_path)\n",
        "        text_model, bert_tokenizer = load_bert_model(bert_weight_path)\n",
        "\n",
        "        # === STEP 2: Data split ===\n",
        "        texts_train_val, texts_test, images_train_val, images_test, labels_train_val, labels_test = train_test_split(\n",
        "            texts, images, labels, test_size=0.15, stratify=labels, random_state=seed\n",
        "        )\n",
        "        texts_train, texts_val, images_train, images_val, labels_train, labels_val = train_test_split(\n",
        "            texts_train_val, images_train_val, labels_train_val, test_size=0.176, stratify=labels_train_val, random_state=seed\n",
        "        )\n",
        "\n",
        "        # === STEP 3: Create datasets and dataloaders ===\n",
        "        train_dataset = MultiModalDataset(images_train, texts_train, labels_train, bert_tokenizer, train_transform, 512)\n",
        "        val_dataset = MultiModalDataset(images_val, texts_val, labels_val, bert_tokenizer, val_test_transform, 512)\n",
        "        test_dataset = MultiModalDataset(images_test, texts_test, labels_test, bert_tokenizer, val_test_transform, 512)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, worker_init_fn=seed_worker)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, worker_init_fn=seed_worker)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, worker_init_fn=seed_worker)\n",
        "\n",
        "        # === STEP 4: Initialize model and training tools ===\n",
        "        # Initialize model, criterion, optimizer, and device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = MultiModalModel(densenet_model, deit_model, text_model, fc_network).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
        "\n",
        "        # ✅ Optimizer: only update trainable parameters (attention + fc)\n",
        "        optimizer = Adam(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=1e-5, weight_decay=1e-4\n",
        "        )\n",
        "\n",
        "        scheduler = StepLR(optimizer, step_size=6, gamma=0.1)\n",
        "\n",
        "        best_val_f1 = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Store loss history\n",
        "        mm_train_losses = []\n",
        "        mm_val_losses = []\n",
        "\n",
        "        for epoch in range(100):\n",
        "            train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "            val_loss, val_acc, val_report = evaluate_model(model, val_loader, criterion, device)\n",
        "            val_f1 = val_report['macro avg']['f1-score']\n",
        "\n",
        "            mm_train_losses.append(train_loss)\n",
        "            mm_val_losses.append(val_loss)\n",
        "\n",
        "            print(f\"Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "            if val_f1 > best_val_f1:\n",
        "                best_val_f1 = val_f1\n",
        "                torch.save(model.state_dict(), f\"best_model_seed_{seed}.bin\")\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= 10:\n",
        "                    print(\"Early stopping.\")\n",
        "                    break\n",
        "\n",
        "        # Load best model and evaluate on test\n",
        "        model.load_state_dict(torch.load(f\"best_model_seed_{seed}.bin\"))\n",
        "\n",
        "        # Modify the call to test_model to also return the results_df\n",
        "        # Assuming test_model now returns (results_df, classification_report)\n",
        "        results_df_test, test_report = test_model(model, test_loader, criterion, device)\n",
        "\n",
        "        all_val_reports.append(val_report)\n",
        "        all_test_reports.append(test_report)\n",
        "\n",
        "        # Extract AUC-ROC score from results_df_test and append to its dedicated list\n",
        "        # Assuming AUC-ROC is a single value in the DataFrame, likely in the first row\n",
        "        if not results_df_test.empty and 'AUC-ROC' in results_df_test.columns:\n",
        "            auc_roc_score = results_df_test['AUC-ROC'].iloc[0]\n",
        "            all_test_auc_roc_scores.append(auc_roc_score)\n",
        "        else:\n",
        "            print(f\"Warning: AUC-ROC score not found in results_df_test for seed {seed}\")\n",
        "            all_test_auc_roc_scores.append(None) # Append None or handle as appropriate\n",
        "\n",
        "\n",
        "        # Plot training and validation loss curves for this seed\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(range(1, len(mm_train_losses) + 1), mm_train_losses, label='Training Loss')\n",
        "        plt.plot(range(1, len(mm_val_losses) + 1), mm_val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Training and Validation Loss (Seed {seed})')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Return all collected reports and the new AUC-ROC scores list|\n",
        "    return all_val_reports, all_test_reports, all_test_auc_roc_scores"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T06:00:30.147397Z",
          "iopub.execute_input": "2025-06-18T06:00:30.148084Z",
          "iopub.status.idle": "2025-06-18T06:00:30.161149Z",
          "shell.execute_reply.started": "2025-06-18T06:00:30.148061Z",
          "shell.execute_reply": "2025-06-18T06:00:30.16027Z"
        },
        "id": "RlayZK74HkRd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "seed_list = [42, 77, 7, 101, 314, 2024, 123, 88, 11, 999]\n",
        "val_results, test_results, test_auc_roc = run_experiments_over_seeds(seed_list)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T06:00:35.22489Z",
          "iopub.execute_input": "2025-06-18T06:00:35.225563Z",
          "iopub.status.idle": "2025-06-18T06:41:22.603851Z",
          "shell.execute_reply.started": "2025-06-18T06:00:35.225538Z",
          "shell.execute_reply": "2025-06-18T06:41:22.603262Z"
        },
        "id": "CxGTkfGUHkRd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "-C6-bplGSqB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "macro_f1_scores = []\n",
        "accuracies = []\n",
        "macro_precisions = []\n",
        "macro_recalls = []\n",
        "auc_roc_scores = []\n",
        "\n",
        "for report in test_results:\n",
        "    macro_f1_scores.append(report[\"macro avg\"][\"f1-score\"])\n",
        "    macro_precisions.append(report[\"macro avg\"][\"precision\"])\n",
        "    macro_recalls.append(report[\"macro avg\"][\"recall\"])\n",
        "    accuracies.append(report[\"accuracy\"])\n",
        "\n",
        "# Assuming test_auc_roc is a list of AUC-ROC scores, one for each seed run\n",
        "auc_roc_scores = test_auc_roc\n",
        "\n",
        "# Print extracted values, now including AUC-ROC\n",
        "print(\"===== Individual Seed Run Results =====\")\n",
        "for i, (acc, f1, prec, rec, roc_auc) in enumerate(zip(accuracies, macro_f1_scores, macro_precisions, macro_recalls, auc_roc_scores), 1):\n",
        "    print(f\"Seed Run {i}:\")\n",
        "    print(f\"  Accuracy        : {acc:.4f}\")\n",
        "    print(f\"  Macro F1-score  : {f1:.4f}\")\n",
        "    print(f\"  Macro Precision : {prec:.4f}\")\n",
        "    print(f\"  Macro Recall    : {rec:.4f}\")\n",
        "    print(f\"  AUC-ROC         : {roc_auc:.4f}\")\n",
        "    print()\n",
        "\n",
        "# --- Bootstrap Confidence Interval Calculation ---\n",
        "\n",
        "# Create a DataFrame from your collected metric lists\n",
        "# This DataFrame will serve the same purpose as final_results_df_pt for the bootstrap function\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Accuracy': accuracies,\n",
        "    'Precision': macro_precisions, # Assuming you want macro precision for CI\n",
        "    'Recall': macro_recalls,     # Assuming you want macro recall for CI\n",
        "    'F1-Score': macro_f1_scores, # Assuming you want macro F1 for CI\n",
        "    'AUC-ROC': auc_roc_scores\n",
        "})\n",
        "\n",
        "# Define metrics for bootstrap (ensure these match the DataFrame column names)\n",
        "metrics_to_bootstrap = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
        "\n",
        "# Bootstrapped CI function (re-defined here for clarity, or ensure it's globally accessible)\n",
        "def bootstrap_ci(data, n_bootstrap=10000, ci=95):\n",
        "    data = np.array(data)\n",
        "    means = []\n",
        "    n = len(data)\n",
        "    if n == 0: # Handle empty data case\n",
        "        return np.nan, np.nan, np.nan\n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(data, size=n, replace=True)\n",
        "        means.append(np.mean(sample))\n",
        "    lower = np.percentile(means, (100 - ci) / 2)\n",
        "    upper = 100 - (100 - ci) / 2\n",
        "    if ci == 95: # Handle cases where upper bound may be 95% if lower bound is 2.5%\n",
        "        upper_percentile = 97.5\n",
        "    elif ci == 90:\n",
        "        upper_percentile = 95\n",
        "    else:\n",
        "        upper_percentile = 100 - (100 - ci) / 2\n",
        "    upper = np.percentile(means, upper_percentile)\n",
        "    return np.mean(means), lower, upper\n",
        "\n",
        "# Prepare summary with Mean, Std, and Bootstrap CI\n",
        "summary_rows = []\n",
        "for metric in metrics_to_bootstrap:\n",
        "    mean_val = metrics_df[metric].mean()\n",
        "    std_val = metrics_df[metric].std()\n",
        "    boot_mean, ci_lower, ci_upper = bootstrap_ci(metrics_df[metric].dropna().values) # Handle potential NaNs\n",
        "\n",
        "    summary_rows.append({\n",
        "        'Metric': metric,\n",
        "        'Mean': mean_val,\n",
        "        'Std Dev': std_val,\n",
        "        'Boot Mean': boot_mean,\n",
        "        '95% CI Lower': ci_lower,\n",
        "        '95% CI Upper': ci_upper\n",
        "    })\n",
        "\n",
        "# Final summary table as a Pandas DataFrame\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "print(\"---\") # Horizontal line for separation\n",
        "print(\"===== Aggregated Results (Mean ± Std & Bootstrap 95% CI) =====\")\n",
        "\n",
        "# Print the summary DataFrame formatted for readability\n",
        "# You can customize this printing more, e.g., using to_string(index=False) or f-strings\n",
        "for _, row in summary_df.iterrows():\n",
        "    print(f\"{row['Metric']}:\")\n",
        "    print(f\"  Mean ± Std       : {row['Mean']:.4f} ± {row['Std Dev']:.4f}\")\n",
        "    print(f\"  95% CI (Bootstrap): [{row['95% CI Lower']:.4f}, {row['95% CI Upper']:.4f}]\")\n",
        "    print()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-18T06:46:01.668592Z",
          "iopub.execute_input": "2025-06-18T06:46:01.66888Z",
          "iopub.status.idle": "2025-06-18T06:46:02.750922Z",
          "shell.execute_reply.started": "2025-06-18T06:46:01.668861Z",
          "shell.execute_reply": "2025-06-18T06:46:02.750327Z"
        },
        "id": "0k3tijPAHkRd"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}