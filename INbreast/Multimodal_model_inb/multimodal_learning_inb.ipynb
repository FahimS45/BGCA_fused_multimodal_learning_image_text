{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-22T09:37:21.564751Z",
     "iopub.status.busy": "2025-06-22T09:37:21.564232Z",
     "iopub.status.idle": "2025-06-22T09:37:29.891558Z",
     "shell.execute_reply": "2025-06-22T09:37:29.890777Z",
     "shell.execute_reply.started": "2025-06-22T09:37:21.564728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet152\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, DeiTForImageClassification\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:37:29.893469Z",
     "iopub.status.busy": "2025-06-22T09:37:29.892786Z",
     "iopub.status.idle": "2025-06-22T09:37:29.898755Z",
     "shell.execute_reply": "2025-06-22T09:37:29.898096Z",
     "shell.execute_reply.started": "2025-06-22T09:37:29.893445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:37:29.900081Z",
     "iopub.status.busy": "2025-06-22T09:37:29.899532Z",
     "iopub.status.idle": "2025-06-22T09:37:29.980552Z",
     "shell.execute_reply": "2025-06-22T09:37:29.979931Z",
     "shell.execute_reply.started": "2025-06-22T09:37:29.900056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:37:29.998570Z",
     "iopub.status.busy": "2025-06-22T09:37:29.998287Z",
     "iopub.status.idle": "2025-06-22T09:37:30.014791Z",
     "shell.execute_reply": "2025-06-22T09:37:30.013977Z",
     "shell.execute_reply.started": "2025-06-22T09:37:29.998546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_dir, target_size=(224, 224)):\n",
    "\n",
    "    X = []\n",
    "\n",
    "    # Sort filenames to maintain the default order\n",
    "    for img_name in sorted(os.listdir(data_dir)):\n",
    "        img_path = os.path.join(data_dir, img_name)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert('RGB')  \n",
    "                img = img.resize(target_size)  \n",
    "                X.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "\n",
    "    return X\n",
    "\n",
    "data_dir = '/kaggle/input/inbreast/INbreast_malignant_non-malignant_screened/INbreast_malignant_non-malignant_screened'\n",
    "X = load_and_preprocess_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:43:41.513296Z",
     "iopub.status.busy": "2025-06-22T09:43:41.512986Z",
     "iopub.status.idle": "2025-06-22T09:43:41.519163Z",
     "shell.execute_reply": "2025-06-22T09:43:41.518271Z",
     "shell.execute_reply.started": "2025-06-22T09:43:41.513274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "file_path = \"/kaggle/input/inbreast/multimodal_data_inbreast_text.csv\"\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_report_id(filename):\n",
    "    # Split by underscores and extract the second part\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) > 2:\n",
    "        return parts[1]\n",
    "    return None\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df['report_id'] = df['File Name'].apply(extract_report_id)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:43:45.425890Z",
     "iopub.status.busy": "2025-06-22T09:43:45.425149Z",
     "iopub.status.idle": "2025-06-22T09:43:45.503464Z",
     "shell.execute_reply": "2025-06-22T09:43:45.502705Z",
     "shell.execute_reply.started": "2025-06-22T09:43:45.425849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encode class labels\n",
    "label_mapping = {'Non-malignant': 0, 'Malignant': 1}\n",
    "labels = df['Class'].map(label_mapping).values\n",
    "texts = df['Structured Text'].tolist()\n",
    "group_ids = df['report_id'].tolist()\n",
    "\n",
    "# Convert everything to NumPy arrays to ensure consistent indexing\n",
    "images = np.stack([np.array(img) for img in X])\n",
    "texts = np.array(texts)\n",
    "labels = np.array(labels)\n",
    "group_ids = np.array(group_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array if not already\n",
    "labels_train = np.array(labels_train)\n",
    "\n",
    "# Count class occurrences\n",
    "num_class_0 = np.sum(labels_train == 0)\n",
    "num_class_1 = np.sum(labels_train == 1)\n",
    "\n",
    "print(f\"Number of class 0 (Non-Malignant): {num_class_0}\")\n",
    "print(f\"Number of class 1 (Malignant): {num_class_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of top discriminative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:43:53.265234Z",
     "iopub.status.busy": "2025-06-22T09:43:53.264692Z",
     "iopub.status.idle": "2025-06-22T09:43:53.284474Z",
     "shell.execute_reply": "2025-06-22T09:43:53.283772Z",
     "shell.execute_reply.started": "2025-06-22T09:43:53.265209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_discriminative_words_all_classes(texts, labels, top_n=20, custom_exclusions={'no'}):\n",
    "    custom_stopwords = ENGLISH_STOP_WORDS.difference(custom_exclusions)\n",
    "\n",
    "    # Tokenize and clean text by class\n",
    "    class_tokens = {0: [], 1: []}\n",
    "    for text, label in zip(texts, labels):\n",
    "        words = text.lower().split()\n",
    "        words = [word.strip('.,():;-\"\\n') for word in words if word not in custom_stopwords]\n",
    "        class_tokens[label].extend(words)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq_0 = Counter(class_tokens[0])\n",
    "    freq_1 = Counter(class_tokens[1])\n",
    "\n",
    "    removed_words_by_class = {0: set(), 1: set()}\n",
    "\n",
    "    for target_class in [0, 1]:\n",
    "        target_freq = freq_0 if target_class == 0 else freq_1\n",
    "        other_freq = freq_1 if target_class == 0 else freq_0\n",
    "\n",
    "        discriminative_scores = {}\n",
    "        for word, freq in target_freq.items():\n",
    "            if freq > 1:\n",
    "                score = freq / (1 + other_freq.get(word, 0))\n",
    "                discriminative_scores[word] = score\n",
    "\n",
    "        top_words = sorted(discriminative_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        top_discriminative_words = {word for word, _ in top_words}\n",
    "        removed_words_by_class[target_class] = top_discriminative_words\n",
    "\n",
    "    # Combine all removed words\n",
    "    all_removed_words = removed_words_by_class[0].union(removed_words_by_class[1])\n",
    "\n",
    "    # Remove all discriminative words from texts\n",
    "    texts_filtered = []\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        cleaned_words = [\n",
    "            word for word in words\n",
    "            if word.lower().strip('.,():;-\"\\n') not in all_removed_words\n",
    "        ]\n",
    "        cleaned_text = ' '.join(cleaned_words)\n",
    "        texts_filtered.append(cleaned_text)\n",
    "\n",
    "    return texts_filtered, removed_words_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=torch.unique(torch.tensor(labels)).numpy(),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# Move class weights to the same device as the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "print(f\"Class Weights on Device: {class_weights_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:04.354954Z",
     "iopub.status.busy": "2025-06-22T09:44:04.354650Z",
     "iopub.status.idle": "2025-06-22T09:44:04.362673Z",
     "shell.execute_reply": "2025-06-22T09:44:04.361849Z",
     "shell.execute_reply.started": "2025-06-22T09:44:04.354932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, image_data, text_data, labels, tokenizer, transform=None, max_text_length=512):\n",
    "        self.image_data = image_data\n",
    "        self.text_data = text_data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_text_length = max_text_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --- Image data ---\n",
    "        image = self.image_data[idx]\n",
    "\n",
    "        # Convert NumPy array to PIL Image if needed\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:  # Grayscale: H x W\n",
    "                image = Image.fromarray(image.astype(np.uint8), mode='L')\n",
    "            elif image.ndim == 3:  # Color: H x W x C\n",
    "                image = Image.fromarray(image.astype(np.uint8))\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # --- Text data ---\n",
    "        text = self.text_data[idx]\n",
    "        encoded_text = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_text[\"input_ids\"].flatten()\n",
    "        attention_mask = encoded_text[\"attention_mask\"].flatten()\n",
    "\n",
    "        # --- Label ---\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:06.982722Z",
     "iopub.status.busy": "2025-06-22T09:44:06.982439Z",
     "iopub.status.idle": "2025-06-22T09:44:06.989407Z",
     "shell.execute_reply": "2025-06-22T09:44:06.988535Z",
     "shell.execute_reply.started": "2025-06-22T09:44:06.982701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GatedCrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim, hidden_dim):\n",
    "        super(GatedCrossAttention, self).__init__()\n",
    "        self.query_proj = nn.Linear(query_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(context_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(context_dim, hidden_dim)\n",
    "\n",
    "        # Gating mechanism\n",
    "        self.gate_fc = nn.Linear(query_dim + hidden_dim, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        Q = self.query_proj(query).unsqueeze(1)     # [B, 1, H]\n",
    "        K = self.key_proj(context).unsqueeze(1)     # [B, 1, H]\n",
    "        V = self.value_proj(context).unsqueeze(1)   # [B, 1, H]\n",
    "\n",
    "        attn_scores = torch.bmm(Q, K.transpose(1, 2))  # [B, 1, 1]\n",
    "        attn_weights = self.softmax(attn_scores)       # [B, 1, 1]\n",
    "        attended = torch.bmm(attn_weights, V).squeeze(1)  # [B, H]\n",
    "\n",
    "        # Project query into hidden space for fusion\n",
    "        query_proj = self.query_proj(query)  # [B, H]\n",
    "\n",
    "        # Gate computation\n",
    "        gate_input = torch.cat([query, attended], dim=1)  # [B, Q+H]\n",
    "        gate = self.sigmoid(self.gate_fc(gate_input))     # [B, H]\n",
    "\n",
    "        # Gated fusion\n",
    "        gated_output = gate * query_proj + (1 - gate) * attended  # [B, H]\n",
    "        return gated_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:11.564013Z",
     "iopub.status.busy": "2025-06-22T09:44:11.563716Z",
     "iopub.status.idle": "2025-06-22T09:44:11.570813Z",
     "shell.execute_reply": "2025-06-22T09:44:11.570157Z",
     "shell.execute_reply.started": "2025-06-22T09:44:11.563990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiModalModelGatedCrossAttention(nn.Module):\n",
    "    def __init__(self, resnet_model, deit_model, text_model, fc_network):\n",
    "        super(MultiModalModelGatedCrossAttention, self).__init__()\n",
    "        self.resnet_model = resnet_model\n",
    "        self.deit_model = deit_model\n",
    "        self.text_model = text_model\n",
    "\n",
    "        self.resnet_dim = 2048\n",
    "        self.deit_dim = 768\n",
    "        self.text_dim = 768\n",
    "\n",
    "        self.fc_network = fc_network\n",
    "\n",
    "        self.vision_dim = self.resnet_dim + self.deit_dim\n",
    "        self.hidden_dim = 512  # Fusion hidden space\n",
    "\n",
    "        # Gated cross attention: both directions\n",
    "        self.text_to_vision = GatedCrossAttention(self.text_dim, self.vision_dim, self.hidden_dim)\n",
    "        self.vision_to_text = GatedCrossAttention(self.vision_dim, self.text_dim, self.hidden_dim)\n",
    "\n",
    "    def forward(self, image_input, input_ids, attention_mask):\n",
    "        # Extract image features\n",
    "        resnet_features = self.resnet_model(image_input)                  # [B, 2048]\n",
    "        deit_features = self.deit_model(image_input).logits              # [B, 768]\n",
    "        vision_features = torch.cat([resnet_features, deit_features], dim=1)  # [B, 2816]\n",
    "\n",
    "        # Extract text features from [CLS] token of last hidden state\n",
    "        text_outputs = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        text_features = text_outputs.hidden_states[-1][:, 0, :]          # [B, 768]\n",
    "\n",
    "        # Gated Cross Attention both directions\n",
    "        enhanced_text = self.text_to_vision(text_features, vision_features)    # [B, 512]\n",
    "        enhanced_vision = self.vision_to_text(vision_features, text_features)  # [B, 512]\n",
    "\n",
    "        # Final fused representation\n",
    "        fused = torch.cat([enhanced_text, enhanced_vision], dim=1)       # [B, 1024]\n",
    "\n",
    "        output = self.fc_network(fused)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:14.041818Z",
     "iopub.status.busy": "2025-06-22T09:44:14.041164Z",
     "iopub.status.idle": "2025-06-22T09:44:14.050625Z",
     "shell.execute_reply": "2025-06-22T09:44:14.049783Z",
     "shell.execute_reply.started": "2025-06-22T09:44:14.041789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move data to the device\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Step the scheduler based on the validation loss (if using ReduceLROnPlateau)\n",
    "    # scheduler.step(epoch_loss)  # Uncomment if using ReduceLROnPlateau\n",
    "\n",
    "    # Step the scheduler every epoch (if using StepLR or similar)\n",
    "    scheduler.step()\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:16.115512Z",
     "iopub.status.busy": "2025-06-22T09:44:16.114755Z",
     "iopub.status.idle": "2025-06-22T09:44:16.121708Z",
     "shell.execute_reply": "2025-06-22T09:44:16.120838Z",
     "shell.execute_reply.started": "2025-06-22T09:44:16.115484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            # Move data to the device\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate metrics\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels)\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    true_labels = torch.stack(true_labels).cpu()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_accuracy, classification_report(\n",
    "        true_labels, predictions, target_names=df['Class'].unique(), output_dict=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:18.124165Z",
     "iopub.status.busy": "2025-06-22T09:44:18.123625Z",
     "iopub.status.idle": "2025-06-22T09:44:18.129929Z",
     "shell.execute_reply": "2025-06-22T09:44:18.129060Z",
     "shell.execute_reply.started": "2025-06-22T09:44:18.124140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(true_labels, predictions, save_cm_path, class_names=['Non-malignant', 'Malignant']):\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(save_cm_path), exist_ok=True)\n",
    "\n",
    "    # Generate Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, \n",
    "                yticklabels=class_names, linewidths=2, cbar=False, square=True, annot_kws={\"size\": 14})\n",
    "    \n",
    "    plt.xlabel('Predicted Labels', fontsize=12)\n",
    "    plt.ylabel('True Labels', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "\n",
    "    # Save confusion matrix as PDF\n",
    "    plt.savefig(save_cm_path, bbox_inches=\"tight\", format=\"pdf\")\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved as {save_cm_path}\")\n",
    "\n",
    "    # Return classification report\n",
    "    return classification_report(true_labels, predictions, target_names=class_names, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:19.899224Z",
     "iopub.status.busy": "2025-06-22T09:44:19.898512Z",
     "iopub.status.idle": "2025-06-22T09:44:19.908996Z",
     "shell.execute_reply": "2025-06-22T09:44:19.908338Z",
     "shell.execute_reply.started": "2025-06-22T09:44:19.899200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, val_loader, criterion, device, seed=None, report_save_path=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluation\"):\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            predictions.extend(preds.cpu())\n",
    "            true_labels.extend(labels.cpu())\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            probabilities.extend(probs.cpu())\n",
    "\n",
    "    predictions = torch.stack(predictions)\n",
    "    true_labels = torch.stack(true_labels)\n",
    "    probabilities = torch.stack(probabilities)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    auc_roc = roc_auc_score(true_labels, probabilities[:, 1], multi_class=\"ovr\") if probabilities.shape[1] > 1 else None\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    if auc_roc is not None:\n",
    "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    save_cm_path = '/kaggle/working/confusion_matrix.pdf'\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Non-malignant', 'Malignant'],\n",
    "                yticklabels=['Non-malignant', 'Malignant'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(save_cm_path)\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    class_report = classification_report(\n",
    "        true_labels,\n",
    "        predictions,\n",
    "        target_names=['Non-malignant', 'Malignant'],\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    # Return metrics summary and full classification report\n",
    "    results_df = pd.DataFrame([{\n",
    "        'Seed': seed if seed is not None else 'N/A',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': auc_roc\n",
    "    }])\n",
    "\n",
    "    return results_df, class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:23.491280Z",
     "iopub.status.busy": "2025-06-22T09:44:23.490985Z",
     "iopub.status.idle": "2025-06-22T09:44:23.496244Z",
     "shell.execute_reply": "2025-06-22T09:44:23.495580Z",
     "shell.execute_reply.started": "2025-06-22T09:44:23.491259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)           \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T09:44:28.209814Z",
     "iopub.status.busy": "2025-06-22T09:44:28.209548Z",
     "iopub.status.idle": "2025-06-22T09:44:28.226143Z",
     "shell.execute_reply": "2025-06-22T09:44:28.225581Z",
     "shell.execute_reply.started": "2025-06-22T09:44:28.209793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define model dimensions\n",
    "input_dim = 1024  # Refined features\n",
    "hidden_dim = 512\n",
    "output_dim = 2  # Number of classes (Non-malignant, Malignant)\n",
    "\n",
    "# Initialize the fully connected network\n",
    "fc_network = FullyConnectedNetwork(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal learning (ResNet152+Deit+BioClinicalBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet152 model\n",
    "def load_resnet_model(weight_path):\n",
    "\n",
    "    resnet_model = resnet152(pretrained=False)\n",
    "    num_features = resnet_model.fc.in_features\n",
    "    resnet_model.fc = torch.nn.Identity()  \n",
    "    \n",
    "    # Load the state_dict\n",
    "    state_dict = torch.load(weight_path)\n",
    "    \n",
    "    # Remove \"fc.weight\" and \"fc.bias\" from the state_dict\n",
    "    state_dict = {k: v for k, v in state_dict.items() if not k.startswith(\"fc.\")}\n",
    "    \n",
    "    # Load the pruned state_dict into the model\n",
    "    resnet_model.load_state_dict(state_dict, strict=False)\n",
    "    resnet_model.eval()  \n",
    "    \n",
    "    for param in resnet_model.parameters():\n",
    "        param.requires_grad = False  \n",
    "    \n",
    "    return resnet_model\n",
    "\n",
    "\n",
    "# Load the pre-trained DeiT model\n",
    "def load_deit_model(weight_path, num_classes=2):\n",
    "\n",
    "    deit_model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n",
    "    \n",
    "    # Modify the classifier layer\n",
    "    deit_model.classifier = nn.Identity()  \n",
    "\n",
    "    # Load trained weights\n",
    "    state_dict = torch.load(weight_path, map_location=torch.device('cpu'))\n",
    "    deit_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    deit_model.eval()\n",
    "\n",
    "    # Freeze parameters\n",
    "    for param in deit_model.parameters():\n",
    "        param.requires_grad = False  \n",
    "\n",
    "    return deit_model\n",
    "\n",
    "\n",
    "# Load the pre-trained BioClinicalBert model\n",
    "def load_bert_model(weight_path, bert_model_name='emilyalsentzer/Bio_ClinicalBERT'):\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    \n",
    "    # Initialize the model with the same configuration used during training\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=2)  \n",
    "\n",
    "    # Load the state_dict into the model\n",
    "    state_dict = torch.load(weight_path, map_location=torch.device('cpu'))  \n",
    "\n",
    "    # Load the state dict into the model (ignore mismatched keys if any)\n",
    "    bert_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    bert_model.eval()\n",
    "\n",
    "    # Freeze the parameters of the model (for feature extraction)\n",
    "    for param in bert_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    return bert_model, tokenizer\n",
    "\n",
    "# Paths to saved model weights\n",
    "resnet_weight_path = \"/kaggle/input/resnet152-model-inbreast/pytorch/best_resnet152_model.pth\"\n",
    "deit_weight_path = \"/kaggle/input/deit-model-adasyn-inbreast/pytorch/best_deit_model.pth\"\n",
    "bert_weight_path = \"/kaggle/input/bioclinicalbert-model-inbreast/pytorch/best_bio_clinical_model_state_v4.bin\"\n",
    "\n",
    "# Load the pretrained models for feature extraction\n",
    "resnet_model = load_resnet_model(resnet_weight_path)\n",
    "deit_model = load_deit_model(deit_weight_path)\n",
    "text_model, bert_tokenizer = load_bert_model(bert_weight_path)\n",
    "\n",
    "print(\"Models loaded and ready for feature extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test across 10 different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T10:19:20.207283Z",
     "iopub.status.busy": "2025-06-22T10:19:20.206524Z",
     "iopub.status.idle": "2025-06-22T10:19:20.223190Z",
     "shell.execute_reply": "2025-06-22T10:19:20.222424Z",
     "shell.execute_reply.started": "2025-06-22T10:19:20.207238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_experiments_over_seeds(seed_list):\n",
    "    all_val_reports = []\n",
    "    all_test_reports = []\n",
    "    # Initialize a list to store AUC-ROC scores for each seed\n",
    "    all_test_auc_roc_scores = []\n",
    "    \n",
    "    for seed in seed_list:\n",
    "        print(f\"\\n====== Running for Seed: {seed} ======\")\n",
    "        \n",
    "        # Set seed\n",
    "        set_seed(seed)\n",
    "        def seed_worker(worker_id): np.random.seed(seed); random.seed(seed)\n",
    "\n",
    "        # Reload all models fresh for this seed\n",
    "        resnet_model = load_resnet_model(resnet_weight_path)\n",
    "        deit_model = load_deit_model(deit_weight_path)\n",
    "        text_model, bert_tokenizer = load_bert_model(bert_weight_path)\n",
    "        \n",
    "        # Data splitting\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=seed)\n",
    "        train_val_idx, test_idx = next(gss.split(texts, labels, groups=group_ids))\n",
    "        texts_train_val, texts_test = texts[train_val_idx], texts[test_idx]\n",
    "        images_train_val, images_test = images[train_val_idx], images[test_idx]\n",
    "        labels_train_val, labels_test = labels[train_val_idx], labels[test_idx]\n",
    "        group_ids_train_val = group_ids[train_val_idx]\n",
    "\n",
    "        gss2 = GroupShuffleSplit(n_splits=1, test_size=0.176, random_state=seed)\n",
    "        train_idx, val_idx = next(gss2.split(texts_train_val, labels_train_val, groups=group_ids_train_val))\n",
    "        texts_train, texts_val = texts_train_val[train_idx], texts_train_val[val_idx]\n",
    "        images_train, images_val = images_train_val[train_idx], images_train_val[val_idx]\n",
    "        labels_train, labels_val = labels_train_val[train_idx], labels_train_val[val_idx]\n",
    "\n",
    "        # Clean training texts\n",
    "        texts_train_filtered, removed_words_by_class = filter_discriminative_words_all_classes(\n",
    "            texts_train, labels_train, top_n=7\n",
    "        )\n",
    "\n",
    "        # You can inspect the removed words:\n",
    "        print(\"\\nSummary of Removed Words by Class:\")\n",
    "        for cls, words in removed_words_by_class.items():\n",
    "            print(f\"Class {cls}: {sorted(words)}\")\n",
    "\n",
    "        # Datasets\n",
    "        train_dataset = MultiModalDataset(images_train, texts_train_filtered, labels_train, bert_tokenizer, train_transform)\n",
    "        val_dataset   = MultiModalDataset(images_val, texts_val, labels_val, bert_tokenizer, val_test_transform)\n",
    "        test_dataset  = MultiModalDataset(images_test, texts_test, labels_test, bert_tokenizer, val_test_transform)\n",
    "\n",
    "        # Loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, worker_init_fn=seed_worker)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, worker_init_fn=seed_worker)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "        # Model and training setup\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = MultiModalModelGatedCrossAttention(resnet_model, deit_model, text_model, fc_network).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "        optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-6, weight_decay=1e-4)\n",
    "        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Store loss history\n",
    "        mm_train_losses = []\n",
    "        mm_val_losses = []\n",
    "\n",
    "        for epoch in range(100):\n",
    "            train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "            val_loss, val_acc, val_report = evaluate_model(model, val_loader, criterion, device)\n",
    "            val_f1 = val_report['macro avg']['f1-score']\n",
    "\n",
    "            mm_train_losses.append(train_loss)\n",
    "            mm_val_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1:03d} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                torch.save(model.state_dict(), f\"best_model_seed_{seed}.bin\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 10:\n",
    "                    print(\"Early stopping.\")\n",
    "                    break\n",
    "\n",
    "        # Load best model and evaluate on test\n",
    "        model.load_state_dict(torch.load(f\"best_model_seed_{seed}.bin\"))\n",
    "        \n",
    "        # Modify the call to test_model to also return the results_df\n",
    "        # Assuming test_model now returns (results_df, classification_report)\n",
    "        results_df_test, test_report = test_model(model, test_loader, criterion, device)\n",
    "\n",
    "        all_val_reports.append(val_report)\n",
    "        all_test_reports.append(test_report)\n",
    "        \n",
    "        # Extract AUC-ROC score from results_df_test and append to its dedicated list\n",
    "        # Assuming AUC-ROC is a single value in the DataFrame, likely in the first row\n",
    "        if not results_df_test.empty and 'AUC-ROC' in results_df_test.columns:\n",
    "            auc_roc_score = results_df_test['AUC-ROC'].iloc[0]\n",
    "            all_test_auc_roc_scores.append(auc_roc_score)\n",
    "        else:\n",
    "            print(f\"Warning: AUC-ROC score not found in results_df_test for seed {seed}\")\n",
    "            all_test_auc_roc_scores.append(None) # Append None or handle as appropriate\n",
    "            \n",
    "\n",
    "        # Plot training and validation loss curves for this seed\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(mm_train_losses) + 1), mm_train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(mm_val_losses) + 1), mm_val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training and Validation Loss (Seed {seed})')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Return all collected reports and the new AUC-ROC scores list\n",
    "    return all_val_reports, all_test_reports, all_test_auc_roc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [42, 77, 7, 101, 314, 2024, 123, 88, 11, 999]\n",
    "val_results, test_results, test_auc_roc = run_experiments_over_seeds(seed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(data, n_bootstrap=10000, ci=95):\n",
    "    data = np.array(data)\n",
    "    means = []\n",
    "    n = len(data)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(data, size=n, replace=True)\n",
    "        means.append(np.mean(sample))\n",
    "    lower = np.percentile(means, (100 - ci) / 2)\n",
    "    upper_percentile = 100 - (100 - ci) / 2\n",
    "    upper = np.percentile(means, upper_percentile)\n",
    "    return np.mean(means), lower, upper\n",
    "\n",
    "def evaluate_metrics_with_bootstrap(test_reports, auc_roc_scores, n_bootstrap=10000, ci=95):\n",
    "    \"\"\"\n",
    "    Computes mean, std, and bootstrap confidence intervals for classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "        test_reports (list): List of classification reports (dicts) from sklearn.\n",
    "        auc_roc_scores (list): List of AUC-ROC scores (float).\n",
    "        n_bootstrap (int): Number of bootstrap samples.\n",
    "        ci (int): Confidence interval percentage (default 95).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary table with Mean, Std Dev, Bootstrapped CI for each metric.\n",
    "    \"\"\"\n",
    "    macro_f1_scores = []\n",
    "    accuracies = []\n",
    "    macro_precisions = []\n",
    "    macro_recalls = []\n",
    "\n",
    "    for report in test_reports:\n",
    "        macro_f1_scores.append(report[\"macro avg\"][\"f1-score\"])\n",
    "        macro_precisions.append(report[\"macro avg\"][\"precision\"])\n",
    "        macro_recalls.append(report[\"macro avg\"][\"recall\"])\n",
    "        accuracies.append(report[\"accuracy\"])\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Accuracy': accuracies,\n",
    "        'Precision': macro_precisions,\n",
    "        'Recall': macro_recalls,\n",
    "        'F1-Score': macro_f1_scores,\n",
    "        'AUC-ROC': auc_roc_scores\n",
    "    })\n",
    "\n",
    "    metrics_to_bootstrap = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "\n",
    "    summary_rows = []\n",
    "    for metric in metrics_to_bootstrap:\n",
    "        values = metrics_df[metric].dropna().values\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        boot_mean, ci_lower, ci_upper = bootstrap_ci(values, n_bootstrap=n_bootstrap, ci=ci)\n",
    "        summary_rows.append({\n",
    "            'Metric': metric,\n",
    "            'Mean': mean_val,\n",
    "            'Std Dev': std_val,\n",
    "            'Boot Mean': boot_mean,\n",
    "            f'{ci}% CI Lower': ci_lower,\n",
    "            f'{ci}% CI Upper': ci_upper\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "    # Print individual results\n",
    "    print(\"===== Individual Seed Run Results =====\")\n",
    "    for i, (acc, f1, prec, rec, roc_auc) in enumerate(zip(\n",
    "        metrics_df['Accuracy'], metrics_df['F1-Score'], metrics_df['Precision'],\n",
    "        metrics_df['Recall'], metrics_df['AUC-ROC']), 1):\n",
    "        print(f\"Seed Run {i}:\")\n",
    "        print(f\"  Accuracy        : {acc:.4f}\")\n",
    "        print(f\"  Macro F1-score  : {f1:.4f}\")\n",
    "        print(f\"  Macro Precision : {prec:.4f}\")\n",
    "        print(f\"  Macro Recall    : {rec:.4f}\")\n",
    "        print(f\"  AUC-ROC         : {roc_auc:.4f}\\n\")\n",
    "\n",
    "    # Print the aggregated summary\n",
    "    print(\"===== Aggregated Results (Mean ± Std & Bootstrap CI) =====\")\n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"{row['Metric']}:\")\n",
    "        print(f\"  Mean ± Std        : {row['Mean']:.4f} ± {row['Std Dev']:.4f}\")\n",
    "        print(f\"  {ci}% CI (Bootstrap): [{row[f'{ci}% CI Lower']:.4f}, {row[f'{ci}% CI Upper']:.4f}]\\n\")\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summary_df = evaluate_metrics_with_bootstrap(test_reports=test_results, auc_roc_scores=test_auc_roc)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6221207,
     "sourceId": 11998202,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 203644,
     "modelInstanceId": 181411,
     "sourceId": 212835,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 203645,
     "modelInstanceId": 181412,
     "sourceId": 219071,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 203187,
     "modelInstanceId": 180953,
     "sourceId": 445303,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 203181,
     "modelInstanceId": 180946,
     "sourceId": 445312,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 203185,
     "modelInstanceId": 180951,
     "sourceId": 445321,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 203178,
     "modelInstanceId": 180943,
     "sourceId": 445322,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
