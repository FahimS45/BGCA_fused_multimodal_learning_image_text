# Enhancing Breast Cancer Diagnosis through Multimodal Fusion of Features Extracted Using Deep Learning Models and Large Language Models

This repository contains the codebase for our research paper, "Enhancing Breast Cancer Diagnosis through Multimodal Fusion of Features Extracted Using Deep Learning Models and Large Language Models." 

## Abstract
Breast cancer remains the second leading cause of cancer fatalities in women, and while AI excels in unimodal learning, effective diagnosis often demands a multimodal perspective of patient data. This study proposes an ideal multimodal methodology that integrates breast medical images with natural-text reports derived from structured clinical tables to classify non-malignant or malignant cases confidently. For image data, five Convolutional Neural Networks (CNNs) and two Vision Transformers (ViTs) were evaluated with and without extensive data optimization techniques. The best-performing models were used for vision feature extraction and combined through an ensemble approach. Transformed textual data were then processed using four state-of-the-art Large Language Models (LLMs) and fused with the vision features through a Bidirectional Gated Cross-Attention (BGCA) mechanism. To the best of our knowledge, we are the first to explore metadata transformation into natural, context-specific sentences within the breast cancer domain. Evaluation was conducted on three mammography datasets: DMID, INbreast, and MIAS, across ten random seeds. The proposed method consistently outperformed unimodal baselines, achieving up to 96.36\% accuracy and 95.38\% macro F1 on DMID, along with a 99.74\% AUC on INbreast. When tested on the combined distribution, 94.54\% macro F1, and 98.90\% AUC were observed. Comprehensive ablation studies, statistical testing, and cross-dataset validation confirmed the robustness of our framework. In a semi-supervised domain adaptation setting, incorporating only 10\% of the target domain data led to an 8.7\% improvement in F1-score. Furthermore, model predictions were interpreted using Grad-CAM and SHAP to enhance transparency. The full codebase is available on GitHub.

## System Architecture
The core of our methodology is a multimodal learning framework that effectively combines visual and textual features for enhanced diagnostic accuracy. The system architecture, as detailed in the diagram below, outlines the key stages of our approach:
![System Architecture](assets/Final_architecture.png)

