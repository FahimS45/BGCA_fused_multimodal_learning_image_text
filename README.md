Enhancing Breast Cancer Diagnosis through Multimodal Fusion of Features Extracted Using Deep Learning Models and Large Language Models
This repository contains the codebase for our research paper, "Enhancing Breast Cancer Diagnosis through Multimodal Fusion of Features Extracted Using Deep Learning Models and Large Language Models," submitted to a top-tier Q1 journal. The project focuses on developing an innovative multimodal approach to breast cancer diagnosis by integrating medical images with textual patient reports.

Abstract
Breast cancer continues to pose a significant health risk, remaining the second largest contributor to cancer fatalities in women. While AI has achieved impressive results in unimodal learning, effective clinical diagnosis frequently relies on a more comprehensive, multimodal perspective of patient data. In this study, we developed an ideal multimodal methodology that integrates breast medical images with natural-text reports derived from structured clinical tables to classify non-malignant or malignant cases confidently. For image data, five Convolutional Neural Networks (CNNs) and two Vision Transformers (ViTs) were evaluated with and without extensive data optimization techniques. The best-performing models were used for vision feature extraction and combined through an ensemble approach. Transformed textual data were then processed using four state-of-the-art Large Language Models (LLMs) and fused with the vision features through a Bidirectional Gated Cross-Attention (BGCA) mechanism. To the best of our knowledge, we are the first to transform metadata into natural domain-specific sentences within the breast cancer domain. Evaluation was conducted on three benchmark mammography datasets: DMID, INbreast, and MIAS. The proposed method consistently outperformed unimodal baselines, achieving up to 96.36% accuracy and 95.38% macro F1 on DMID, along with a 99.74% AUC on INbreast. When tested on the combined distribution, it maintained strong results: 94.54% macro F1, and 98.90% AUC. Comprehensive ablation studies, statistical testing, and cross-dataset validation further confirmed the robustness of our framework. Finally, in a semi-supervised domain adaptation setting, adding just 10% of target domain data improved F1 by 8.7%. The full codebase is publicly available on GitHub.

System Architecture
The core of our methodology is a multimodal learning framework that effectively combines visual and textual features for enhanced diagnostic accuracy. The system architecture, as detailed in the diagram below, outlines the key stages of our approach:
